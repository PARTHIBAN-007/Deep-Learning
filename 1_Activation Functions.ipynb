{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T13:30:42.742024Z","iopub.status.busy":"2024-08-28T13:30:42.741547Z","iopub.status.idle":"2024-08-28T13:30:42.748238Z","shell.execute_reply":"2024-08-28T13:30:42.746903Z","shell.execute_reply.started":"2024-08-28T13:30:42.741979Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\",category = FutureWarning)\n","\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["# Activation Function"]},{"cell_type":"markdown","metadata":{},"source":["An activation function is a mathematical function used in neural networks to introduce non-linearity into the model. This non-linearity is crucial because it allows the neural network to learn and model complex patterns in the data. Without an activation function, a neural network would be just a linear model, regardless of the number of layers it has."]},{"cell_type":"markdown","metadata":{},"source":["## Sigmoid Activation Function"]},{"cell_type":"markdown","metadata":{},"source":["**Formula:**\n","\n","$$\n","\\sigma(x) = \\frac{1}{1 + e^{-x}}\n","$$\n","\n","**Range:**\n","\n","$$\n","\\sigma(x) \\in (0, 1)\n","$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T14:22:08.976944Z","iopub.status.busy":"2024-08-28T14:22:08.976397Z","iopub.status.idle":"2024-08-28T14:22:09.688043Z","shell.execute_reply":"2024-08-28T14:22:09.686713Z","shell.execute_reply.started":"2024-08-28T14:22:08.976897Z"},"trusted":true},"outputs":[],"source":["# sigoid Activation Function\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","x = np.linspace(-10, 10, 400) \n","y = sigmoid(x) \n","\n","sns.set(style=\"white\")\n","fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Plot sigmoid function\n","sns.lineplot(x=x, y=y, ax=ax, color='green', label='Sigmoid Function ')\n","plt.title('Sigmoid Activation Function')\n","plt.legend()\n","\n","\n","# Set the axes to cross at (0,0)\n","ax.spines['left'].set_position('zero')\n","ax.spines['bottom'].set_position('zero')\n","ax.spines['right'].set_color('none')\n","ax.spines['top'].set_color('none')\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Uses**:\n","\n","Sigmoid Activation Function is mainly used for Binary Classsification\n","\n","\n","\n"," **Problems**:\n","\n","Vanishing Gradient: For large positive or negative values of x, the gradient becomes very small, which can slow down learning.\n","\n","Not Zero-Centered: Output values are always positive, which can lead to inefficient gradient updates.\n","\n","Computationally Expensive: Requires the computation of the exponential function, which can be less efficient."]},{"cell_type":"markdown","metadata":{},"source":["## Tanh Function\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","**Formula:**\n","\n","$$\n","\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n","$$\n","\n","**Range:**\n","\n","$$\n","\\tanh(x) \\in (-1, 1)\n","$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T14:03:15.907425Z","iopub.status.busy":"2024-08-28T14:03:15.906974Z","iopub.status.idle":"2024-08-28T14:03:16.480393Z","shell.execute_reply":"2024-08-28T14:03:16.478950Z","shell.execute_reply.started":"2024-08-28T14:03:15.907379Z"},"trusted":true},"outputs":[],"source":["def tanh(x):\n","    return np.tanh(x)\n","\n","# Generate input data\n","y = tanh(x)\n","\n","# Create a seaborn style plot\n","sns.set(style=\"white\")\n","fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Plot tanh function\n","sns.lineplot(x=x, y=y, ax=ax, color='green', label='Tanh Function')\n","plt.title('Tanh Activation Function')\n","plt.legend()\n","\n","\n","# Set the axes to cross at (0,0)\n","ax.spines['left'].set_position('zero')\n","ax.spines['bottom'].set_position('zero')\n","ax.spines['right'].set_color('none')\n","ax.spines['top'].set_color('none')\n","\n","fig.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Uses:**\n","\n","Used in hidden layers of neural networks as it maps values between -1 and 1, making the output zero-centered.\n","Useful for regression tasks and certain types of recurrent neural networks.\n","\n","\n","**Problems**:\n","\n","Vanishing Gradient: Like sigmoid, tanh suffers from vanishing gradients for large values of x, which can hamper learning in deep networks.\n","\n","Computational Cost: Involves the computation of exponential functions, which can be computationally expensive."]},{"cell_type":"markdown","metadata":{},"source":["## ReLU Function\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","**Formula:**\n","\n","$$\n","\\text{ReLU}(x) = \\max(0, x)\n","$$\n","\n","**Range:**\n","\n","$$\n","\\text{ReLU}(x) \\in [0, \\infty)\n","$$\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T14:02:51.535211Z","iopub.status.busy":"2024-08-28T14:02:51.533908Z","iopub.status.idle":"2024-08-28T14:02:52.062504Z","shell.execute_reply":"2024-08-28T14:02:52.061183Z","shell.execute_reply.started":"2024-08-28T14:02:51.535137Z"},"trusted":true},"outputs":[],"source":["def relu(x):\n","    return np.maximum(0, x)\n","\n","# Generate input data\n","y = relu(x)\n","\n","# Create a seaborn style plot\n","sns.set(style=\"white\")\n","fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Plot ReLU function\n","sns.lineplot(x=x, y=y, ax=ax, color='green', label='ReLU Function ')\n","plt.title('ReLU Activation Function')\n","plt.legend()\n","\n","\n","# Set the axes to cross at (0,0)\n","ax.spines['left'].set_position('zero')\n","ax.spines['bottom'].set_position('zero')\n","ax.spines['right'].set_color('none')\n","ax.spines['top'].set_color('none')\n","\n","\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Uses:**\n","\n","Widely used in hidden layers of neural networks for deep learning due to its simplicity and efficiency.\n","\n","Helps with the convergence of training by mitigating the vanishing gradient problem.\n","\n","**Problems:**\n","\n","Dying ReLU Problem: Neurons can sometimes get stuck in the inactive state (outputting zero) and stop learning if the input is always negative.(Dead Neuron)\n","\n","Not Zero-Centered: Outputs are always non-negative, which can impact learning dynamics."]},{"cell_type":"markdown","metadata":{},"source":["## Leaky ReLU Function\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","**Formula:**\n","\n","$$\n","\\text{Leaky ReLU}(x) = \\begin{cases}\n","x & \\text{if } x > 0 \\\\\n","\\alpha x & \\text{if } x \\leq 0\n","\\end{cases}\n","$$\n","\n","**Range:**\n","\n","$$\n","\\text{Leaky ReLU}(x) \\in (-\\infty, \\infty)\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T14:03:46.073994Z","iopub.status.busy":"2024-08-28T14:03:46.072676Z","iopub.status.idle":"2024-08-28T14:03:46.644756Z","shell.execute_reply":"2024-08-28T14:03:46.643401Z","shell.execute_reply.started":"2024-08-28T14:03:46.073942Z"},"trusted":true},"outputs":[],"source":["def leaky_relu(x, alpha=0.01):\n","    return np.where(x > 0, x, alpha * x)\n","\n","# Generate input data\n","alpha = 0.01\n","y = leaky_relu(x, alpha)\n","\n","# Create a seaborn style plot\n","sns.set(style=\"white\")\n","fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Plot Leaky ReLU function\n","sns.lineplot(x=x, y=y, ax=ax, color='green' , label='Leaky ReLU Function')\n","plt.title('Leaky ReLU Activation Function')\n","plt.legend()\n","\n","# Set the axes to cross at (0,0)\n","ax.spines['left'].set_position('zero')\n","ax.spines['bottom'].set_position('zero')\n","ax.spines['right'].set_color('none')\n","ax.spines['top'].set_color('none')\n","\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Uses:**\n","\n","A variant of ReLU designed to address the dying ReLU problem by allowing a small, non-zero gradient when \n","x is negative.\n","\n","Used in hidden layers of deep networks.\n","\n","**Problems:**\n","\n","Hyperparameter Sensitivity: The choice of ð›¼ can affect performance and requires tuning.\n","\n","Can Still Suffer from Dead Neurons: Although less likely than with standard ReLU."]},{"cell_type":"markdown","metadata":{},"source":["## PReLU Function\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","**Formula:**\n","\n","$$\n","\\text{PReLU}(x) = \\begin{cases}\n","x & \\text{if } x > 0 \\\\\n","\\alpha x & \\text{if } x \\leq 0\n","\\end{cases}\n","$$\n","\n","**Range:**\n","\n","$$\n","\\text{PReLU}(x) \\in (-\\infty, \\infty)\n","$$\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T14:03:52.672324Z","iopub.status.busy":"2024-08-28T14:03:52.671744Z","iopub.status.idle":"2024-08-28T14:03:53.321062Z","shell.execute_reply":"2024-08-28T14:03:53.319716Z","shell.execute_reply.started":"2024-08-28T14:03:52.672268Z"},"trusted":true},"outputs":[],"source":["def prelu(x, alpha):\n","    return np.where(x > 0, x, alpha * x)\n","\n","# Generate input data\n","alpha = 0.01\n","y = prelu(x, alpha)\n","\n","# Create a seaborn style plot\n","sns.set(style=\"white\")\n","fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Plot PReLU function\n","sns.lineplot(x=x, y=y, ax=ax, color='green', label='PReLU Function')\n","plt.title('PReLU Activation Function')\n","plt.legend() \n","\n","# Set the axes to cross at (0,0)\n","ax.spines['left'].set_position('zero')\n","ax.spines['bottom'].set_position('zero')\n","ax.spines['right'].set_color('none')\n","ax.spines['top'].set_color('none')\n","\n","fig.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Uses:**\n","\n","Similar to Leaky ReLU but with the additional flexibility of learning the \n","Î± parameter, which can improve performance in some cases.\n","\n","**Problems:**\n","\n","Increased Complexity: The model becomes more complex due to the additional learnable parameters.\n","\n","Overfitting Risk: The added flexibility might lead to overfitting if not managed properly.\n"]},{"cell_type":"markdown","metadata":{},"source":["## ELU Function\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","\n","**Formula:**\n","\n","$$\n","\\text{ELU}(x) = \\begin{cases}\n","x & \\text{if } x > 0 \\\\\n","\\alpha (e^x - 1) & \\text{if } x \\leq 0\n","\\end{cases}\n","$$\n","\n","**Range:**\n","\n","$$\n","\\text{ELU}(x) \\in (-\\alpha, \\infty)\n","$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T14:04:04.689034Z","iopub.status.busy":"2024-08-28T14:04:04.687757Z","iopub.status.idle":"2024-08-28T14:04:05.311095Z","shell.execute_reply":"2024-08-28T14:04:05.309910Z","shell.execute_reply.started":"2024-08-28T14:04:04.688982Z"},"trusted":true},"outputs":[],"source":["def elu(x, alpha=1.0):\n","    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n","\n","# Generate input data\n","alpha = 1.0\n","y = elu(x, alpha)\n","\n","# Create a seaborn style plot\n","sns.set(style=\"white\")\n","fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Plot ELU function\n","sns.lineplot(x=x, y=y, ax=ax, color='green', label='ELU Function')\n","plt.title('ELU Activation Function')\n","plt.legend()\n","\n","# Set the axes to cross at (0,0)\n","ax.spines['left'].set_position('zero')\n","ax.spines['bottom'].set_position('zero')\n","ax.spines['right'].set_color('none')\n","ax.spines['top'].set_color('none')\n","\n","\n","fig.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Uses:**\n","\n","Addresses vanishing gradient issues and provides smoother gradients compared to ReLU.\n","\n","Useful in deep networks where the output needs to be zero-centered.\n","\n","**Problems:**\n","\n","Computational Cost: Involves the computation of exponential functions, which can be expensive.\n","\n","Negative Output Issue: For negative inputs, the output can become very large if Î± is large."]},{"cell_type":"markdown","metadata":{},"source":["## Softmax Function\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","**Formula:**\n","\n","$$\n","\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n","$$\n","\n","**Range:**\n","\n","$$\n","\\text{Softmax}(x_i) \\in (0, 1)\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T14:04:16.397894Z","iopub.status.busy":"2024-08-28T14:04:16.397433Z","iopub.status.idle":"2024-08-28T14:04:16.994151Z","shell.execute_reply":"2024-08-28T14:04:16.992979Z","shell.execute_reply.started":"2024-08-28T14:04:16.397817Z"},"trusted":true},"outputs":[],"source":["def softmax(x):\n","    e_x = np.exp(x - np.max(x))  \n","\n","    return e_x / e_x.sum(axis=0)\n","\n","# Generate input data\n","x = np.linspace(-2, 2, 100)\n","x = np.vstack([x, np.ones_like(x)]) \n","y = softmax(x)\n","\n","# Create a seaborn style plot\n","sns.set(style=\"white\")\n","fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Plot Softmax function\n","ax.plot(x[0], y[0], color='green', label='Softmax Function')\n","plt.title('Softmax Activation Function')\n","plt.legend()\n","\n","\n","# Set the axes to cross at (0,0)\n","ax.spines['left'].set_position('zero')\n","ax.spines['bottom'].set_position('zero')\n","ax.spines['right'].set_color('none')\n","ax.spines['top'].set_color('none')\n","\n","\n","fig.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Uses:**\n","\n","Used in the output layer of multi-class classification problems to produce a probability distribution over classes.\n","\n","**Problems:**\n","\n","Numerical Stability: Can suffer from numerical instability if not computed carefully, especially for very large or very small inputs.\n","\n","Not Suitable for Hidden Layers: Primarily used in the output layer for classification tasks, not for hidden layers."]},{"cell_type":"markdown","metadata":{},"source":["## Swish Function\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","**Formula:**\n","\n","$$\n","\\text{Swish}(x) = x \\cdot \\sigma(x) = x \\cdot \\frac{1}{1 + e^{-x}}\n","$$\n","\n","**Range:**\n","\n","$$\n","\\text{Swish}(x) \\in (-\\infty, \\infty)\n","$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T14:04:58.391303Z","iopub.status.busy":"2024-08-28T14:04:58.390901Z","iopub.status.idle":"2024-08-28T14:04:59.022743Z","shell.execute_reply":"2024-08-28T14:04:59.021500Z","shell.execute_reply.started":"2024-08-28T14:04:58.391265Z"},"trusted":true},"outputs":[],"source":["def swish(x):\n","    return x * sigmoid(x)\n","\n","# Generate input data\n","x = np.linspace(-10, 10, 400)\n","y = swish(x)\n","\n","# Create a seaborn style plot\n","sns.set(style=\"white\")\n","fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Plot Swish function\n","sns.lineplot(x=x, y=y, ax=ax,color= 'green' , label='Swish Function')\n","plt.title('Swish Activation Function')\n","plt.legend()\n","\n","\n","# Set the axes to cross at (0,0)\n","ax.spines['left'].set_position('zero')\n","ax.spines['bottom'].set_position('zero')\n","ax.spines['right'].set_color('none')\n","ax.spines['top'].set_color('none')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["**Uses:**\n","\n","Can improve training dynamics and performance in some deep learning models.\n","\n","Smooth activation function that is continuous and differentiable.\n","\n","\n","**Problems:**\n","\n","Computational Cost: Involves both multiplication and sigmoid computation, which can be more expensive than ReLU.\n","\n","Less Proven: While it has shown benefits in some cases, it is not as widely tested or used as other activation functions."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
