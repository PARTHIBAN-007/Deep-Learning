{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\",category = FutureWarning)\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-08-28T13:30:42.741547Z","iopub.execute_input":"2024-08-28T13:30:42.742024Z","iopub.status.idle":"2024-08-28T13:30:42.748238Z","shell.execute_reply.started":"2024-08-28T13:30:42.741979Z","shell.execute_reply":"2024-08-28T13:30:42.746903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Activation Function","metadata":{}},{"cell_type":"markdown","source":"An activation function is a mathematical function used in neural networks to introduce non-linearity into the model. This non-linearity is crucial because it allows the neural network to learn and model complex patterns in the data. Without an activation function, a neural network would be just a linear model, regardless of the number of layers it has.","metadata":{}},{"cell_type":"markdown","source":"## Sigmoid Activation Function","metadata":{}},{"cell_type":"markdown","source":"**Formula:**\n\n$$\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n$$\n\n**Range:**\n\n$$\n\\sigma(x) \\in (0, 1)\n$$\n","metadata":{}},{"cell_type":"code","source":"# sigoid Activation Function\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nx = np.linspace(-10, 10, 400) \ny = sigmoid(x) \n\nsns.set(style=\"white\")\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot sigmoid function\nsns.lineplot(x=x, y=y, ax=ax, color='green', label='Sigmoid Function ')\nplt.title('Sigmoid Activation Function')\nplt.legend()\n\n\n# Set the axes to cross at (0,0)\nax.spines['left'].set_position('zero')\nax.spines['bottom'].set_position('zero')\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:22:08.976397Z","iopub.execute_input":"2024-08-28T14:22:08.976944Z","iopub.status.idle":"2024-08-28T14:22:09.688043Z","shell.execute_reply.started":"2024-08-28T14:22:08.976897Z","shell.execute_reply":"2024-08-28T14:22:09.686713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Uses**:\n\nSigmoid Activation Function is mainly used for Binary Classsification\n\n\n\n **Problems**:\n\nVanishing Gradient: For large positive or negative values of x, the gradient becomes very small, which can slow down learning.\n\nNot Zero-Centered: Output values are always positive, which can lead to inefficient gradient updates.\n\nComputationally Expensive: Requires the computation of the exponential function, which can be less efficient.","metadata":{}},{"cell_type":"markdown","source":"## Tanh Function\n","metadata":{}},{"cell_type":"markdown","source":"\n**Formula:**\n\n$$\n\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n$$\n\n**Range:**\n\n$$\n\\tanh(x) \\in (-1, 1)\n$$\n","metadata":{}},{"cell_type":"code","source":"def tanh(x):\n    return np.tanh(x)\n\n# Generate input data\ny = tanh(x)\n\n# Create a seaborn style plot\nsns.set(style=\"white\")\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot tanh function\nsns.lineplot(x=x, y=y, ax=ax, color='green', label='Tanh Function')\nplt.title('Tanh Activation Function')\nplt.legend()\n\n\n# Set the axes to cross at (0,0)\nax.spines['left'].set_position('zero')\nax.spines['bottom'].set_position('zero')\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\n\nfig.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:03:15.906974Z","iopub.execute_input":"2024-08-28T14:03:15.907425Z","iopub.status.idle":"2024-08-28T14:03:16.480393Z","shell.execute_reply.started":"2024-08-28T14:03:15.907379Z","shell.execute_reply":"2024-08-28T14:03:16.478950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Uses:**\n\nUsed in hidden layers of neural networks as it maps values between -1 and 1, making the output zero-centered.\nUseful for regression tasks and certain types of recurrent neural networks.\n\n\n**Problems**:\n\nVanishing Gradient: Like sigmoid, tanh suffers from vanishing gradients for large values of x, which can hamper learning in deep networks.\n\nComputational Cost: Involves the computation of exponential functions, which can be computationally expensive.","metadata":{}},{"cell_type":"markdown","source":"## ReLU Function\n","metadata":{}},{"cell_type":"markdown","source":"\n**Formula:**\n\n$$\n\\text{ReLU}(x) = \\max(0, x)\n$$\n\n**Range:**\n\n$$\n\\text{ReLU}(x) \\in [0, \\infty)\n$$\n\n","metadata":{}},{"cell_type":"code","source":"def relu(x):\n    return np.maximum(0, x)\n\n# Generate input data\ny = relu(x)\n\n# Create a seaborn style plot\nsns.set(style=\"white\")\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot ReLU function\nsns.lineplot(x=x, y=y, ax=ax, color='green', label='ReLU Function ')\nplt.title('ReLU Activation Function')\nplt.legend()\n\n\n# Set the axes to cross at (0,0)\nax.spines['left'].set_position('zero')\nax.spines['bottom'].set_position('zero')\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\n\n\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:02:51.533908Z","iopub.execute_input":"2024-08-28T14:02:51.535211Z","iopub.status.idle":"2024-08-28T14:02:52.062504Z","shell.execute_reply.started":"2024-08-28T14:02:51.535137Z","shell.execute_reply":"2024-08-28T14:02:52.061183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Uses:**\n\nWidely used in hidden layers of neural networks for deep learning due to its simplicity and efficiency.\n\nHelps with the convergence of training by mitigating the vanishing gradient problem.\n\n**Problems:**\n\nDying ReLU Problem: Neurons can sometimes get stuck in the inactive state (outputting zero) and stop learning if the input is always negative.(Dead Neuron)\n\nNot Zero-Centered: Outputs are always non-negative, which can impact learning dynamics.","metadata":{}},{"cell_type":"markdown","source":"## Leaky ReLU Function\n","metadata":{}},{"cell_type":"markdown","source":"\n**Formula:**\n\n$$\n\\text{Leaky ReLU}(x) = \\begin{cases}\nx & \\text{if } x > 0 \\\\\n\\alpha x & \\text{if } x \\leq 0\n\\end{cases}\n$$\n\n**Range:**\n\n$$\n\\text{Leaky ReLU}(x) \\in (-\\infty, \\infty)\n$$","metadata":{}},{"cell_type":"code","source":"def leaky_relu(x, alpha=0.01):\n    return np.where(x > 0, x, alpha * x)\n\n# Generate input data\nalpha = 0.01\ny = leaky_relu(x, alpha)\n\n# Create a seaborn style plot\nsns.set(style=\"white\")\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot Leaky ReLU function\nsns.lineplot(x=x, y=y, ax=ax, color='green' , label='Leaky ReLU Function')\nplt.title('Leaky ReLU Activation Function')\nplt.legend()\n\n# Set the axes to cross at (0,0)\nax.spines['left'].set_position('zero')\nax.spines['bottom'].set_position('zero')\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\n\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:03:46.072676Z","iopub.execute_input":"2024-08-28T14:03:46.073994Z","iopub.status.idle":"2024-08-28T14:03:46.644756Z","shell.execute_reply.started":"2024-08-28T14:03:46.073942Z","shell.execute_reply":"2024-08-28T14:03:46.643401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Uses:**\n\nA variant of ReLU designed to address the dying ReLU problem by allowing a small, non-zero gradient when \nx is negative.\n\nUsed in hidden layers of deep networks.\n\n**Problems:**\n\nHyperparameter Sensitivity: The choice of 𝛼 can affect performance and requires tuning.\n\nCan Still Suffer from Dead Neurons: Although less likely than with standard ReLU.","metadata":{}},{"cell_type":"markdown","source":"## PReLU Function\n","metadata":{}},{"cell_type":"markdown","source":"\n**Formula:**\n\n$$\n\\text{PReLU}(x) = \\begin{cases}\nx & \\text{if } x > 0 \\\\\n\\alpha x & \\text{if } x \\leq 0\n\\end{cases}\n$$\n\n**Range:**\n\n$$\n\\text{PReLU}(x) \\in (-\\infty, \\infty)\n$$\n\n","metadata":{}},{"cell_type":"code","source":"def prelu(x, alpha):\n    return np.where(x > 0, x, alpha * x)\n\n# Generate input data\nalpha = 0.01\ny = prelu(x, alpha)\n\n# Create a seaborn style plot\nsns.set(style=\"white\")\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot PReLU function\nsns.lineplot(x=x, y=y, ax=ax, color='green', label='PReLU Function')\nplt.title('PReLU Activation Function')\nplt.legend() \n\n# Set the axes to cross at (0,0)\nax.spines['left'].set_position('zero')\nax.spines['bottom'].set_position('zero')\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\n\nfig.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:03:52.671744Z","iopub.execute_input":"2024-08-28T14:03:52.672324Z","iopub.status.idle":"2024-08-28T14:03:53.321062Z","shell.execute_reply.started":"2024-08-28T14:03:52.672268Z","shell.execute_reply":"2024-08-28T14:03:53.319716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Uses:**\n\nSimilar to Leaky ReLU but with the additional flexibility of learning the \nα parameter, which can improve performance in some cases.\n\n**Problems:**\n\nIncreased Complexity: The model becomes more complex due to the additional learnable parameters.\n\nOverfitting Risk: The added flexibility might lead to overfitting if not managed properly.\n","metadata":{}},{"cell_type":"markdown","source":"## ELU Function\n","metadata":{}},{"cell_type":"markdown","source":"\n\n\n**Formula:**\n\n$$\n\\text{ELU}(x) = \\begin{cases}\nx & \\text{if } x > 0 \\\\\n\\alpha (e^x - 1) & \\text{if } x \\leq 0\n\\end{cases}\n$$\n\n**Range:**\n\n$$\n\\text{ELU}(x) \\in (-\\alpha, \\infty)\n$$\n","metadata":{}},{"cell_type":"code","source":"def elu(x, alpha=1.0):\n    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n\n# Generate input data\nalpha = 1.0\ny = elu(x, alpha)\n\n# Create a seaborn style plot\nsns.set(style=\"white\")\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot ELU function\nsns.lineplot(x=x, y=y, ax=ax, color='green', label='ELU Function')\nplt.title('ELU Activation Function')\nplt.legend()\n\n# Set the axes to cross at (0,0)\nax.spines['left'].set_position('zero')\nax.spines['bottom'].set_position('zero')\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\n\n\nfig.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:04:04.687757Z","iopub.execute_input":"2024-08-28T14:04:04.689034Z","iopub.status.idle":"2024-08-28T14:04:05.311095Z","shell.execute_reply.started":"2024-08-28T14:04:04.688982Z","shell.execute_reply":"2024-08-28T14:04:05.309910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Uses:**\n\nAddresses vanishing gradient issues and provides smoother gradients compared to ReLU.\n\nUseful in deep networks where the output needs to be zero-centered.\n\n**Problems:**\n\nComputational Cost: Involves the computation of exponential functions, which can be expensive.\n\nNegative Output Issue: For negative inputs, the output can become very large if α is large.","metadata":{}},{"cell_type":"markdown","source":"## Softmax Function\n","metadata":{}},{"cell_type":"markdown","source":"\n\n**Formula:**\n\n$$\n\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n$$\n\n**Range:**\n\n$$\n\\text{Softmax}(x_i) \\in (0, 1)\n$$","metadata":{}},{"cell_type":"code","source":"def softmax(x):\n    e_x = np.exp(x - np.max(x))  \n\n    return e_x / e_x.sum(axis=0)\n\n# Generate input data\nx = np.linspace(-2, 2, 100)\nx = np.vstack([x, np.ones_like(x)]) \ny = softmax(x)\n\n# Create a seaborn style plot\nsns.set(style=\"white\")\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot Softmax function\nax.plot(x[0], y[0], color='green', label='Softmax Function')\nplt.title('Softmax Activation Function')\nplt.legend()\n\n\n# Set the axes to cross at (0,0)\nax.spines['left'].set_position('zero')\nax.spines['bottom'].set_position('zero')\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\n\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:04:16.397433Z","iopub.execute_input":"2024-08-28T14:04:16.397894Z","iopub.status.idle":"2024-08-28T14:04:16.994151Z","shell.execute_reply.started":"2024-08-28T14:04:16.397817Z","shell.execute_reply":"2024-08-28T14:04:16.992979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Uses:**\n\nUsed in the output layer of multi-class classification problems to produce a probability distribution over classes.\n\n**Problems:**\n\nNumerical Stability: Can suffer from numerical instability if not computed carefully, especially for very large or very small inputs.\n\nNot Suitable for Hidden Layers: Primarily used in the output layer for classification tasks, not for hidden layers.","metadata":{}},{"cell_type":"markdown","source":"## Swish Function\n","metadata":{}},{"cell_type":"markdown","source":"\n**Formula:**\n\n$$\n\\text{Swish}(x) = x \\cdot \\sigma(x) = x \\cdot \\frac{1}{1 + e^{-x}}\n$$\n\n**Range:**\n\n$$\n\\text{Swish}(x) \\in (-\\infty, \\infty)\n$$\n","metadata":{}},{"cell_type":"code","source":"def swish(x):\n    return x * sigmoid(x)\n\n# Generate input data\nx = np.linspace(-10, 10, 400)\ny = swish(x)\n\n# Create a seaborn style plot\nsns.set(style=\"white\")\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot Swish function\nsns.lineplot(x=x, y=y, ax=ax,color= 'green' , label='Swish Function')\nplt.title('Swish Activation Function')\nplt.legend()\n\n\n# Set the axes to cross at (0,0)\nax.spines['left'].set_position('zero')\nax.spines['bottom'].set_position('zero')\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\n\nplt.tight_layout()\nplt.show()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T14:04:58.390901Z","iopub.execute_input":"2024-08-28T14:04:58.391303Z","iopub.status.idle":"2024-08-28T14:04:59.022743Z","shell.execute_reply.started":"2024-08-28T14:04:58.391265Z","shell.execute_reply":"2024-08-28T14:04:59.021500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Uses:**\n\nCan improve training dynamics and performance in some deep learning models.\n\nSmooth activation function that is continuous and differentiable.\n\n\n**Problems:**\n\nComputational Cost: Involves both multiplication and sigmoid computation, which can be more expensive than ReLU.\n\nLess Proven: While it has shown benefits in some cases, it is not as widely tested or used as other activation functions.","metadata":{}}]}